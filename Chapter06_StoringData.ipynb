{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from urllib.request import urlopen, urlretrieve\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_absolute_url(base_url: str, source: str) -> str:\n",
    "    is_absolute = source.startswith((\"http://\", \"https://\"))\n",
    "    has_external_domain = is_absolute and base_url not in source\n",
    "\n",
    "    if has_external_domain:\n",
    "        return None\n",
    "\n",
    "    if is_absolute:\n",
    "        url = source.replace(\"www.\", \"\")\n",
    "    elif source.startswith(\"www.\"):\n",
    "        url = urljoin(\"http://\", source.replace(\"www.\", \"\"))\n",
    "    else:\n",
    "        url = urljoin(base_url, source)\n",
    "\n",
    "    parsed = urlparse(url)\n",
    "    return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "\n",
    "\n",
    "def get_download_path(base_url: str, absolute_url: str, download_dir: str):\n",
    "    if not base_url.endswith(\"/\"):\n",
    "        base_url += \"/\"\n",
    "\n",
    "    relative_path = absolute_url.replace(base_url, \"\")\n",
    "    download_path = Path(download_dir) / relative_path\n",
    "\n",
    "    if not os.path.exists(download_path.parent):\n",
    "        os.makedirs(download_path.parent)\n",
    "\n",
    "    return download_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dir = \"downloaded\"\n",
    "base_url = \"http://pythonscraping.com\"\n",
    "url = \"http://www.pythonscraping.com\"\n",
    "\n",
    "html = urlopen(url)\n",
    "bs = BeautifulSoup(html, \"html.parser\")\n",
    "download_list = bs.find_all(src=True)\n",
    "\n",
    "for download in download_list:\n",
    "    if file_url := get_absolute_url(base_url, download[\"src\"]):\n",
    "        print(file_url)\n",
    "        download_path = get_download_path(base_url, file_url, download_dir)\n",
    "        urlretrieve(file_url, download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test.csv\", \"w+\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow((\"number\", \"number plus 2\", \"number times 2\"))\n",
    "    for i in range(10):\n",
    "        writer.writerow((i, i + 2, i * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "url = \"http://en.wikipedia.org/wiki/Comparison_of_text_editors\"\n",
    "\n",
    "html = urlopen(url)\n",
    "bs = BeautifulSoup(html, \"html.parser\")\n",
    "table = bs.select(\"table.wikitable\")[0]\n",
    "rows = table.findAll(\"tr\")\n",
    "\n",
    "with open(\"editors.csv\", \"w+\", newline=\"\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    for row in rows:\n",
    "        cells = row.findAll([\"td\", \"th\"])\n",
    "        writer.writerow([cell.get_text().strip() for cell in cells])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import random\n",
    "import pymysql\n",
    "import re\n",
    "\n",
    "conn = pymysql.connect(\n",
    "    host=\"127.0.0.1\",\n",
    "    unix_socket=\"/tmp/mysql.sock\",\n",
    "    user=\"root\",\n",
    "    passwd=\"root\",\n",
    "    db=\"mysql\",\n",
    "    charset=\"utf8\",\n",
    ")\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"USE scraping\")\n",
    "\n",
    "random.seed(datetime.datetime.now())\n",
    "\n",
    "\n",
    "def store(title, content):\n",
    "    cur.execute(\n",
    "        'INSERT INTO pages (title, content) VALUES (\"%s\", \"%s\")',\n",
    "        (title, content),\n",
    "    )\n",
    "    cur.connection.commit()\n",
    "\n",
    "\n",
    "def getLinks(articleUrl):\n",
    "    html = urlopen(\"http://en.wikipedia.org\" + articleUrl)\n",
    "    bs = BeautifulSoup(html, \"html.parser\")\n",
    "    title = bs.find(\"h1\").get_text()\n",
    "    content = bs.find(\"div\", {\"id\": \"mw-content-text\"}).find(\"p\").get_text()\n",
    "    store(title, content)\n",
    "    return bs.find(\"div\", {\"id\": \"bodyContent\"}).findAll(\n",
    "        \"a\", href=re.compile(\"^(/wiki/)((?!:).)*$\")\n",
    "    )\n",
    "\n",
    "\n",
    "links = getLinks(\"/wiki/Kevin_Bacon\")\n",
    "try:\n",
    "    while len(links) > 0:\n",
    "        newArticle = links[random.randint(0, len(links) - 1)].attrs[\"href\"]\n",
    "        print(newArticle)\n",
    "        links = getLinks(newArticle)\n",
    "finally:\n",
    "    cur.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pymysql\n",
    "from random import shuffle\n",
    "\n",
    "conn = pymysql.connect(\n",
    "    host=\"127.0.0.1\",\n",
    "    unix_socket=\"/tmp/mysql.sock\",\n",
    "    user=\"root\",\n",
    "    passwd=\"root\",\n",
    "    db=\"mysql\",\n",
    "    charset=\"utf8\",\n",
    ")\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"USE wikipedia\")\n",
    "\n",
    "\n",
    "def insertPageIfNotExists(url):\n",
    "    cur.execute(\"SELECT * FROM pages WHERE url = %s\", (url))\n",
    "    if cur.rowcount == 0:\n",
    "        cur.execute(\"INSERT INTO pages (url) VALUES (%s)\", (url))\n",
    "        conn.commit()\n",
    "        return cur.lastrowid\n",
    "    else:\n",
    "        return cur.fetchone()[0]\n",
    "\n",
    "\n",
    "def loadPages():\n",
    "    cur.execute(\"SELECT * FROM pages\")\n",
    "    pages = [row[1] for row in cur.fetchall()]\n",
    "    return pages\n",
    "\n",
    "\n",
    "def insertLink(fromPageId, toPageId):\n",
    "    cur.execute(\n",
    "        \"SELECT * FROM links WHERE fromPageId = %s AND toPageId = %s\",\n",
    "        (int(fromPageId), int(toPageId)),\n",
    "    )\n",
    "    if cur.rowcount == 0:\n",
    "        cur.execute(\n",
    "            \"INSERT INTO links (fromPageId, toPageId) VALUES (%s, %s)\",\n",
    "            (int(fromPageId), int(toPageId)),\n",
    "        )\n",
    "        conn.commit()\n",
    "\n",
    "\n",
    "def pageHasLinks(pageId):\n",
    "    cur.execute(\"SELECT * FROM links WHERE fromPageId = %s\", (int(pageId)))\n",
    "    rowcount = cur.rowcount\n",
    "    if rowcount == 0:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def getLinks(pageUrl, recursionLevel, pages):\n",
    "    if recursionLevel > 4:\n",
    "        return\n",
    "\n",
    "    pageId = insertPageIfNotExists(pageUrl)\n",
    "    html = urlopen(\"http://en.wikipedia.org{}\".format(pageUrl))\n",
    "    bs = BeautifulSoup(html, \"html.parser\")\n",
    "    links = bs.findAll(\"a\", href=re.compile(\"^(/wiki/)((?!:).)*$\"))\n",
    "    links = [link.attrs[\"href\"] for link in links]\n",
    "\n",
    "    for link in links:\n",
    "        linkId = insertPageIfNotExists(link)\n",
    "        insertLink(pageId, linkId)\n",
    "        if not pageHasLinks(linkId):\n",
    "            print(\"PAGE HAS NO LINKS: {}\".format(link))\n",
    "            pages.append(link)\n",
    "            getLinks(link, recursionLevel + 1, pages)\n",
    "\n",
    "\n",
    "getLinks(\"/wiki/Kevin_Bacon\", 0, loadPages())\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "msg = MIMEText(\"The body of the email is here\")\n",
    "\n",
    "msg[\"Subject\"] = \"An Email Alert\"\n",
    "msg[\"From\"] = \"ryan@pythonscraping.com\"\n",
    "msg[\"To\"] = \"webmaster@pythonscraping.com\"\n",
    "\n",
    "s = smtplib.SMTP(\"localhost\")\n",
    "s.send_message(msg)\n",
    "s.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import time\n",
    "\n",
    "\n",
    "def sendMail(subject, body):\n",
    "    msg = MIMEText(body)\n",
    "    msg[\"Subject\"] = subject\n",
    "    msg[\"From\"] = \"christmas_alerts@pythonscraping.com\"\n",
    "    msg[\"To\"] = \"ryan@pythonscraping.com\"\n",
    "\n",
    "    s = smtplib.SMTP(\"localhost\")\n",
    "    s.send_message(msg)\n",
    "    s.quit()\n",
    "\n",
    "\n",
    "bs = BeautifulSoup(urlopen(\"https://isitchristmas.com/\"), \"html.parser\")\n",
    "while bs.find(\"a\", {\"id\": \"answer\"}).attrs[\"title\"] == \"NO\":\n",
    "    print(\"It is not Christmas yet.\")\n",
    "    time.sleep(3600)\n",
    "    bs = BeautifulSoup(urlopen(\"https://isitchristmas.com/\"), \"html.parser\")\n",
    "sendMail(\n",
    "    \"It's Christmas!\",\n",
    "    \"According to http://itischristmas.com, it is Christmas!\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
