{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from urllib.request import urlopen, urlretrieve\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_absolute_url(base_url: str, source: str) -> str:\n",
    "    is_absolute = source.startswith((\"http://\", \"https://\"))\n",
    "    has_external_domain = is_absolute and base_url not in source\n",
    "\n",
    "    if has_external_domain:\n",
    "        return None\n",
    "\n",
    "    if is_absolute:\n",
    "        url = source.replace(\"www.\", \"\")\n",
    "    elif source.startswith(\"www.\"):\n",
    "        url = urljoin(\"http://\", source.replace(\"www.\", \"\"))\n",
    "    else:\n",
    "        url = urljoin(base_url, source)\n",
    "\n",
    "    parsed = urlparse(url)\n",
    "    return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "\n",
    "\n",
    "def get_download_path(base_url: str, absolute_url: str, download_dir: str):\n",
    "    if not base_url.endswith(\"/\"):\n",
    "        base_url += \"/\"\n",
    "\n",
    "    relative_path = absolute_url.replace(base_url, \"\")\n",
    "    download_path = Path(download_dir) / relative_path\n",
    "\n",
    "    if not os.path.exists(download_path.parent):\n",
    "        os.makedirs(download_path.parent)\n",
    "\n",
    "    return download_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dir = \"downloaded\"\n",
    "base_url = \"http://pythonscraping.com\"\n",
    "url = \"http://www.pythonscraping.com\"\n",
    "\n",
    "html = urlopen(url)\n",
    "bs = BeautifulSoup(html, \"html.parser\")\n",
    "download_list = bs.find_all(src=True)\n",
    "\n",
    "for download in download_list:\n",
    "    if file_url := get_absolute_url(base_url, download[\"src\"]):\n",
    "        print(file_url)\n",
    "        download_path = get_download_path(base_url, file_url, download_dir)\n",
    "        urlretrieve(file_url, download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test.csv\", \"w+\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow((\"number\", \"number plus 2\", \"number times 2\"))\n",
    "    for i in range(10):\n",
    "        writer.writerow((i, i + 2, i * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "url = \"http://en.wikipedia.org/wiki/Comparison_of_text_editors\"\n",
    "\n",
    "html = urlopen(url)\n",
    "bs = BeautifulSoup(html, \"html.parser\")\n",
    "table = bs.select(\"table.wikitable\")[0]\n",
    "rows = table.findAll(\"tr\")\n",
    "\n",
    "with open(\"editors.csv\", \"w+\", newline=\"\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    for row in rows:\n",
    "        cells = row.findAll([\"td\", \"th\"])\n",
    "        writer.writerow([cell.get_text().strip() for cell in cells])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'Test page title', 'This is some test page content. It can be up to 10,000 characters long.', datetime.datetime(2024, 2, 2, 22, 41, 42))\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "\n",
    "host = \"\"\n",
    "user = \"\"\n",
    "password = \"\"\n",
    "unix_socket = \"\"\n",
    "db = \"\"\n",
    "\n",
    "conn = pymysql.connect(\n",
    "    host=host,\n",
    "    unix_socket=unix_socket,\n",
    "    user=user,\n",
    "    passwd=password,\n",
    "    db=db,\n",
    ")\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"SELECT * FROM pages WHERE id=1\")\n",
    "print(cur.fetchone())\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "(2003, \"Can't connect to MySQL server on '127.0.0.1' ([Errno 2] No such file or directory)\")",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/git/python-scraping/scrapingEnv/lib/python3.12/site-packages/pymysql/connections.py:633\u001b[0m, in \u001b[0;36mConnection.connect\u001b[0;34m(self, sock)\u001b[0m\n\u001b[1;32m    632\u001b[0m sock\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnect_timeout)\n\u001b[0;32m--> 633\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munix_socket\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLocalhost via UNIX socket\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpymysql\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[43mpymysql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m127.0.0.1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43munix_socket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/tmp/mysql.sock\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroot\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpasswd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroot\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmysql\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcharset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m cur \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mcursor()\n\u001b[1;32m     17\u001b[0m cur\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSE scraping\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/git/python-scraping/scrapingEnv/lib/python3.12/site-packages/pymysql/connections.py:358\u001b[0m, in \u001b[0;36mConnection.__init__\u001b[0;34m(self, user, password, host, database, unix_socket, port, charset, collation, sql_mode, read_default_file, conv, use_unicode, client_flag, cursorclass, init_command, connect_timeout, read_default_group, autocommit, local_infile, max_allowed_packet, defer_connect, auth_plugin_map, read_timeout, write_timeout, bind_address, binary_prefix, program_name, server_public_key, ssl, ssl_ca, ssl_cert, ssl_disabled, ssl_key, ssl_verify_cert, ssl_verify_identity, compress, named_pipe, passwd, db)\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 358\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/python-scraping/scrapingEnv/lib/python3.12/site-packages/pymysql/connections.py:711\u001b[0m, in \u001b[0;36mConnection.connect\u001b[0;34m(self, sock)\u001b[0m\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m DEBUG:\n\u001b[1;32m    710\u001b[0m         \u001b[38;5;28mprint\u001b[39m(exc\u001b[38;5;241m.\u001b[39mtraceback)\n\u001b[0;32m--> 711\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# If e is neither DatabaseError or IOError, It's a bug.\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# But raising AssertionError hides original error.\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;66;03m# So just reraise it.\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mOperationalError\u001b[0m: (2003, \"Can't connect to MySQL server on '127.0.0.1' ([Errno 2] No such file or directory)\")"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import random\n",
    "import pymysql\n",
    "import re\n",
    "\n",
    "conn = pymysql.connect(\n",
    "    host=\"127.0.0.1\",\n",
    "    unix_socket=\"/tmp/mysql.sock\",\n",
    "    user=\"root\",\n",
    "    passwd=\"root\",\n",
    "    db=\"mysql\",\n",
    "    charset=\"utf8\",\n",
    ")\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"USE scraping\")\n",
    "\n",
    "random.seed(datetime.datetime.now())\n",
    "\n",
    "\n",
    "def store(title, content):\n",
    "    cur.execute(\n",
    "        'INSERT INTO pages (title, content) VALUES (\"%s\", \"%s\")',\n",
    "        (title, content),\n",
    "    )\n",
    "    cur.connection.commit()\n",
    "\n",
    "\n",
    "def getLinks(articleUrl):\n",
    "    html = urlopen(\"http://en.wikipedia.org\" + articleUrl)\n",
    "    bs = BeautifulSoup(html, \"html.parser\")\n",
    "    title = bs.find(\"h1\").get_text()\n",
    "    content = bs.find(\"div\", {\"id\": \"mw-content-text\"}).find(\"p\").get_text()\n",
    "    store(title, content)\n",
    "    return bs.find(\"div\", {\"id\": \"bodyContent\"}).findAll(\n",
    "        \"a\", href=re.compile(\"^(/wiki/)((?!:).)*$\")\n",
    "    )\n",
    "\n",
    "\n",
    "links = getLinks(\"/wiki/Kevin_Bacon\")\n",
    "try:\n",
    "    while len(links) > 0:\n",
    "        newArticle = links[random.randint(0, len(links) - 1)].attrs[\"href\"]\n",
    "        print(newArticle)\n",
    "        links = getLinks(newArticle)\n",
    "finally:\n",
    "    cur.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pymysql\n",
    "from random import shuffle\n",
    "\n",
    "conn = pymysql.connect(\n",
    "    host=\"127.0.0.1\",\n",
    "    unix_socket=\"/tmp/mysql.sock\",\n",
    "    user=\"root\",\n",
    "    passwd=\"root\",\n",
    "    db=\"mysql\",\n",
    "    charset=\"utf8\",\n",
    ")\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"USE wikipedia\")\n",
    "\n",
    "\n",
    "def insertPageIfNotExists(url):\n",
    "    cur.execute(\"SELECT * FROM pages WHERE url = %s\", (url))\n",
    "    if cur.rowcount == 0:\n",
    "        cur.execute(\"INSERT INTO pages (url) VALUES (%s)\", (url))\n",
    "        conn.commit()\n",
    "        return cur.lastrowid\n",
    "    else:\n",
    "        return cur.fetchone()[0]\n",
    "\n",
    "\n",
    "def loadPages():\n",
    "    cur.execute(\"SELECT * FROM pages\")\n",
    "    pages = [row[1] for row in cur.fetchall()]\n",
    "    return pages\n",
    "\n",
    "\n",
    "def insertLink(fromPageId, toPageId):\n",
    "    cur.execute(\n",
    "        \"SELECT * FROM links WHERE fromPageId = %s AND toPageId = %s\",\n",
    "        (int(fromPageId), int(toPageId)),\n",
    "    )\n",
    "    if cur.rowcount == 0:\n",
    "        cur.execute(\n",
    "            \"INSERT INTO links (fromPageId, toPageId) VALUES (%s, %s)\",\n",
    "            (int(fromPageId), int(toPageId)),\n",
    "        )\n",
    "        conn.commit()\n",
    "\n",
    "\n",
    "def pageHasLinks(pageId):\n",
    "    cur.execute(\"SELECT * FROM links WHERE fromPageId = %s\", (int(pageId)))\n",
    "    rowcount = cur.rowcount\n",
    "    if rowcount == 0:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def getLinks(pageUrl, recursionLevel, pages):\n",
    "    if recursionLevel > 4:\n",
    "        return\n",
    "\n",
    "    pageId = insertPageIfNotExists(pageUrl)\n",
    "    html = urlopen(\"http://en.wikipedia.org{}\".format(pageUrl))\n",
    "    bs = BeautifulSoup(html, \"html.parser\")\n",
    "    links = bs.findAll(\"a\", href=re.compile(\"^(/wiki/)((?!:).)*$\"))\n",
    "    links = [link.attrs[\"href\"] for link in links]\n",
    "\n",
    "    for link in links:\n",
    "        linkId = insertPageIfNotExists(link)\n",
    "        insertLink(pageId, linkId)\n",
    "        if not pageHasLinks(linkId):\n",
    "            print(\"PAGE HAS NO LINKS: {}\".format(link))\n",
    "            pages.append(link)\n",
    "            getLinks(link, recursionLevel + 1, pages)\n",
    "\n",
    "\n",
    "getLinks(\"/wiki/Kevin_Bacon\", 0, loadPages())\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "msg = MIMEText(\"The body of the email is here\")\n",
    "\n",
    "msg[\"Subject\"] = \"An Email Alert\"\n",
    "msg[\"From\"] = \"ryan@pythonscraping.com\"\n",
    "msg[\"To\"] = \"webmaster@pythonscraping.com\"\n",
    "\n",
    "s = smtplib.SMTP(\"localhost\")\n",
    "s.send_message(msg)\n",
    "s.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import time\n",
    "\n",
    "\n",
    "def sendMail(subject, body):\n",
    "    msg = MIMEText(body)\n",
    "    msg[\"Subject\"] = subject\n",
    "    msg[\"From\"] = \"christmas_alerts@pythonscraping.com\"\n",
    "    msg[\"To\"] = \"ryan@pythonscraping.com\"\n",
    "\n",
    "    s = smtplib.SMTP(\"localhost\")\n",
    "    s.send_message(msg)\n",
    "    s.quit()\n",
    "\n",
    "\n",
    "bs = BeautifulSoup(urlopen(\"https://isitchristmas.com/\"), \"html.parser\")\n",
    "while bs.find(\"a\", {\"id\": \"answer\"}).attrs[\"title\"] == \"NO\":\n",
    "    print(\"It is not Christmas yet.\")\n",
    "    time.sleep(3600)\n",
    "    bs = BeautifulSoup(urlopen(\"https://isitchristmas.com/\"), \"html.parser\")\n",
    "sendMail(\n",
    "    \"It's Christmas!\",\n",
    "    \"According to http://itischristmas.com, it is Christmas!\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
